{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatiotemporal Clustering of Land Cover Change \n",
    "\n",
    "The example dataset used in this project is UAV multispectral imagery collected of Niwot Ridge, Colorado collected on a weekly basis between 21 June and 18 August 2017. \n",
    "\n",
    "Niwot Ridge is a highly remote alpine ecosystem, exhibiting interesting dynamics between snow cover and vegetation health. In this example, I extract the Land Cover classification values from rasters from 7 different dates, treating values from each date as an indiviudal column in the training dataset, providing the temporal dimension. The latitude and longitude coordinates are also extracted to include the spatial dimension in the training data.\n",
    "\n",
    "Running clustering this data results in the capture of significant land cover changes that can occur throughout the monitoring period. If data from the first and last date were only observed, such events wouldn't be captured or detected. Ultimately, the output shows regions where significant change has occured at a given point in time, these regions are then labeled with details on the type of change and the date ob which the change occured.\n",
    "\n",
    "Such an approach can be applied to other time series land cover data to identify which time of year/month/week that significant deforestation is occuring for example. Another potential use case is detecting urban land use changes and localizing the area and  time of year they are typically occuring. \n",
    "\n",
    "1. Import Libraries\n",
    "2. Prepare Analysis Files\n",
    "3. Extract Raster Values for Spatiotemporal CLustering\n",
    "4. Spatiotemporal Clustering with KMeans\n",
    "5. Label Land Cover Change Type \n",
    "6. Vectorize Raster Data\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"visualization/NiwotRidge_SpatioTemporal.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rio\n",
    "import os\n",
    "from dask.distributed import Client, Lock\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from lc_spatiotemporal import *\n",
    "import pandas as pd\n",
    "from shapely.geometry import shape\n",
    "import geopandas as gpd\n",
    "from rasterio.features import shapes\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare Analysis Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"output\"\n",
    "\n",
    "lc_files=[f\"{output_dir}/{x}\" for x in os.listdir(\"output\") if \"lc\" in x and x.split(\".\")[-1] == \"tif\"]\n",
    "lc_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract Data from Raster Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medo_\\miniconda3\\envs\\geoai\\Lib\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 57803 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "dim is a required argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# # Read Surface Temperature Band\u001b[39;00m\n\u001b[0;32m     19\u001b[0m lc\u001b[38;5;241m=\u001b[39mrio\u001b[38;5;241m.\u001b[39mopen_rasterio(lc_file, chunks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  lock\u001b[38;5;241m=\u001b[39mLock(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrio-read\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;66;03m# Read LST as DaskArray\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m lc\u001b[38;5;241m=\u001b[39m\u001b[43mlc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate_na\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Add XY Coordinates to dataset for spatial dimension\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Only add once\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m yy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\medo_\\miniconda3\\envs\\geoai\\Lib\\site-packages\\xarray\\core\\dataarray.py:3593\u001b[0m, in \u001b[0;36mDataArray.interpolate_na\u001b[1;34m(self, dim, method, limit, use_coordinate, max_gap, keep_attrs, **kwargs)\u001b[0m\n\u001b[0;32m   3498\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fill in NaNs by interpolating according to different methods.\u001b[39;00m\n\u001b[0;32m   3499\u001b[0m \n\u001b[0;32m   3500\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3589\u001b[0m \u001b[38;5;124;03m  * x        (x) int64 40B 0 1 2 3 4\u001b[39;00m\n\u001b[0;32m   3590\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3591\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interp_na\n\u001b[1;32m-> 3593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minterp_na\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3594\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3598\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_coordinate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_coordinate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_gap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3601\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3602\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\medo_\\miniconda3\\envs\\geoai\\Lib\\site-packages\\xarray\\core\\missing.py:328\u001b[0m, in \u001b[0;36minterp_na\u001b[1;34m(self, dim, use_coordinate, method, limit, max_gap, keep_attrs, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoding\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcftimeindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CFTimeIndex\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim is a required argument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     valids \u001b[38;5;241m=\u001b[39m _get_valid_fill_mask(\u001b[38;5;28mself\u001b[39m, dim, limit)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: dim is a required argument"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the Dask Client\n",
    "\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='10GB')\n",
    "\n",
    "\n",
    "data_vals=list()\n",
    "\n",
    "xx=None\n",
    "yy=None\n",
    "cols=list()\n",
    "\n",
    "\n",
    "for lc_file in lc_files:\n",
    "   \n",
    "    date=lc_file.split(\"/\")[-1].split(\"_\")[0]\n",
    "    name=f\"lc_{date}\"\n",
    "\n",
    "    # # Read Surface Temperature Band\n",
    "    lc=rio.open_rasterio(lc_file, chunks=True,  lock=Lock(\"rio-read\")) # Read Land Cover data as DaskArray\n",
    "\n",
    "    # Add XY Coordinates to dataset for spatial dimension\n",
    "    # Only add once\n",
    "    if xx is None and yy is None:\n",
    "        x=lc.x.values # Get X coordinates from NDVI Raster\n",
    "        y=lc.y.values # Get Y coordinates from NDVI Raster\n",
    "\n",
    "        yy, xx = np.meshgrid(y, x, indexing=\"ij\") # Reshape to grid matching raster dimensions\n",
    "        \n",
    "        # Add coordinates to data list\n",
    "        data_vals.append(xx) \n",
    "        cols.append(\"x\")\n",
    "        data_vals.append(yy)\n",
    "        cols.append(\"y\")\n",
    "\n",
    "    \n",
    "    # Append Land Cover values to data list\n",
    "    data_vals.append(lc.isel(band=0).values)\n",
    "    cols.append(name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Get Raster Dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get Dimensions of the dataset to facilitate reshaping data later on\n",
    "height=lc.rio.height\n",
    "width=lc.rio.width\n",
    "length=height * width\n",
    "n_dims=len(data_vals)\n",
    "orig_shape = (height,width)\n",
    "data_shape = (length,n_dims)\n",
    "res=lc.rio.resolution()\n",
    "cell_area_m2=abs(res[0]) * abs(res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Run KMeans Clustering on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_CLUSTERS= 21\n",
    "\n",
    "\n",
    "# Transpose Extracted raster values into a shape suitable for KMeans\n",
    "## Typically will be reshaped to (width * height, columns) \n",
    "data=np.stack(data_vals)\n",
    "data=data.transpose(1,2,0).reshape(-1, n_dims)\n",
    "data_df=pd.DataFrame(data,columns=cols) # Convert array to Pandas DataFrame, use col names retrieved from the data extraction step\n",
    "\n",
    "mask=data_df.isna().any(axis=1) # Create a mask indicating which rows have NaN \n",
    "data_df.dropna(inplace=True) # Drop NaN to prepare clustering\n",
    "\n",
    "data_df[\"label\"]=kmeans_df(data_df, k=N_CLUSTERS) # Run KMeans clustering on the dataframe\n",
    "data_df=data_df.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Convert Labels to Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Cluster Labels into XR DataArray - use NDVI dimensions to create the raster\n",
    "cluster_map=np.full(length,np.nan)\n",
    "cluster_map[~mask] = data_df.label.to_list()\n",
    "cluster_raster=cluster_map.reshape((height, width))\n",
    "cluster_da=lc.isel(band=0).copy()\n",
    "cluster_da.values=cluster_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Identify Type Land Use Change \n",
    "\n",
    "By taking the mode (most commonly occuring) land cover class in each cluster, we can identify how land use changes over time within that cluster.\n",
    "\n",
    "The lc_change() function detects the date at which the change in land cover occured and returns a label specifying the nature of that change, for example \"Snow to Sparse Vegetation on Week 3\"\n",
    "\n",
    "The land cover change label is then mapped out to the clusters identified by KMeans, after vectorizing the dataset, we can easily visualize this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/classes.json\") as classes:\n",
    "    classes= json.load(classes)\n",
    "\n",
    "class_map=dict(zip(classes.values(), classes.keys()))\n",
    "\n",
    "vars=[x for x in data_df.columns if \"lc\" in x]\n",
    "\n",
    "class_df=data_df[vars].apply(lambda x: x.map(class_map))\n",
    "class_df[\"label\"] = data_df[\"label\"]\n",
    "class_df=class_df.groupby(\"label\").agg(pd.Series.mode).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "dates=[x.split(\"_\")[-1] for x in vars]\n",
    "\n",
    "lc_info_df=pd.DataFrame(class_df[vars].apply(lambda x: lc_change(x.to_list(), time_scale=\"Week\"), axis=1).to_list())\n",
    "lc_info_df[\"label\"] = class_df.label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Vectorize Identified Spatio Temporal Clusters\n",
    "\n",
    "The xr_vectorize function takes an xarray DataArray and converts it to a GeoDataframe vector polygon with matching crs.In the xr_vectorize() function you can also define a threshold for minimum number of pixels and a column name for the raster values passed to the polygon features.\n",
    "\n",
    "Once this step is complete, the GeoDataframe is joined with the land cover change label based on the cluster label as a common index.\n",
    "\n",
    "Lastly, the GeoDataFrame is dissolved, mergine multiple polygon geometries with the same land cover change label. This reduces the total number of polygons and can reduce processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medo_\\miniconda3\\envs\\geoai\\Lib\\site-packages\\xarray\\core\\duck_array_ops.py:215: RuntimeWarning: invalid value encountered in cast\n",
      "  return data.astype(dtype, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "COL_NAME=\"class\"\n",
    "\n",
    "# Vectorize clustered raster to polygon shapes as GeoDataFrame\n",
    "gdf=xr_vectorize(raster=cluster_da, pixel_threshold=0,col_name=COL_NAME)\n",
    "\n",
    "\n",
    "# Join mean values calculated for each cluster - support visualisation\n",
    "gdf=gdf.set_index(COL_NAME).join(lc_info_df.set_index(\"label\"))\n",
    "gdf=gdf[~gdf.label_name.isna()]\n",
    "# gdf=gdf[gdf.label_name != \"No Change\"]\n",
    "\n",
    "gdf_merged=gdf.dissolve(by=COL_NAME, aggfunc=\"first\")\n",
    "gdf_merged=gdf_merged[gdf_merged.label_name != \"No Change\"]\n",
    "\n",
    "# gdf.to_parquet(f\"{output_dir}/kmeans_{N_CLUSTERS}_lc_25cm_.parquet\")\n",
    "gdf_merged.to_parquet(f\"{output_dir}/kmeans_{N_CLUSTERS}_lc_25cm_merged_.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgdf\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39marea_m2\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mpie()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gdf' is not defined"
     ]
    }
   ],
   "source": [
    "gdf.groupby(\"label_name\").area_m2.sum().plot.bar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
